{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a109d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/home/denoyer/workspace/salina'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec926b46",
   "metadata": {},
   "source": [
    "# Loading logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ead582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Read  /home/denoyer/workspace/multirun/2022-01-20/15-57-05/5/ppo_finetune\n",
      "== Read  /home/denoyer/workspace/multirun/2022-01-20/15-57-05/2/ppo_finetune\n",
      "== Read  /home/denoyer/workspace/multirun/2022-01-20/15-57-05/4/ppo_finetune\n",
      "== Read  /home/denoyer/workspace/multirun/2022-01-20/15-57-05/1/ppo_finetune\n",
      "== Read  /home/denoyer/workspace/multirun/2022-01-20/15-57-05/3/ppo_finetune\n",
      "== Read  /home/denoyer/workspace/multirun/2022-01-20/15-57-05/0/ppo_finetune\n",
      "Found 6 logs\n"
     ]
    }
   ],
   "source": [
    "import salina.logger\n",
    "LOGS=salina.logger.read_directory(\"/home/denoyer/workspace/multirun\",use_bz2=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc7db9",
   "metadata": {},
   "source": [
    "# Extracting available scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa2c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  1  unique scenarios\n"
     ]
    }
   ],
   "source": [
    "def extract_scenario(log):\n",
    "    values={}\n",
    "    for k in log.hps:\n",
    "        if k.startswith(\"scenario\"):\n",
    "            values[k]=log.hps[k]\n",
    "    return values\n",
    "\n",
    "def has_scenario(log,scenario):\n",
    "    s=extract_scenario(log)\n",
    "    s=str(s)\n",
    "    return s==str(scenario)\n",
    "    \n",
    "def unique_scenarios(logs):\n",
    "    _unique_scenarios={}\n",
    "    for l in logs.logs:\n",
    "        scenario=extract_scenario(l)\n",
    "        _unique_scenarios[str(scenario)]=scenario\n",
    "    _unique_scenarios=[v for s,v in _unique_scenarios.items()]\n",
    "    return _unique_scenarios\n",
    "print(\"Found \",len(unique_scenarios(LOGS)),\" unique scenarios\")\n",
    "\n",
    "def generate_scenario_html(scenario):\n",
    "    results=[\"<h2>Scenario</h2>\"]\n",
    "    results.append(\"<ul>\")\n",
    "    \n",
    "    for k,v in scenario.items():\n",
    "        results.append(\"<li><b>\"+k+\"</b> =\"+str(v)+\"</li>\")\n",
    "    results.append(\"</ul>\")\n",
    "    return \"\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19202782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hps_html(hps):\n",
    "    results=[\"<h2>Hyper-parameters</h2>\"]\n",
    "    results.append(\"<ul>\")\n",
    "    \n",
    "    for k,v in hps.items():\n",
    "        if k.startswith(\"model\"):\n",
    "            results.append(\"<li><b>\"+k+\"</b> =\"+str(v)+\"</li>\")\n",
    "    results.append(\"</ul>\")\n",
    "    return \"\".join(results)\n",
    "\n",
    "def generate_reward_html(reward_mean,reward_std):\n",
    "    results=[\"<h2>Reward</h2>\"]\n",
    "    results.append(\"<table>\")\n",
    "    n,_=reward_mean.shape\n",
    "    \n",
    "    results.append(\"<tr><td>Task \\\\ Stage </td>\")\n",
    "    for stage in range(n): results.append(\"<td><b>\"+str(stage)+\"</b></td>\")\n",
    "    results.append(\"</tr>\")\n",
    "    \n",
    "    for task in range(n):\n",
    "        results.append(\"<tr><td><b>\"+str(task)+\"</b></td>\")\n",
    "        for stage in range(n): \n",
    "            r=reward_mean[task][stage]\n",
    "            rs=reward_std[task][stage]\n",
    "            results.append(\"<td>\"+str(r)+\"<i>(\"+str(rs)+\")</i></td>\")\n",
    "        results.append(\"</tr>\")\n",
    "    results.append(\"</table>\")\n",
    "    return \"\".join(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc5344",
   "metadata": {},
   "source": [
    "# Extracting unique hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d277328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the run information and extrat the hps as a str in each log\n",
    "import copy\n",
    "def extract_hps(log):\n",
    "    values={}\n",
    "    for k,v in log.hps.items():\n",
    "        if not k==\"model/seed\" and not k.endswith(\"device\"):\n",
    "            values[k]=v\n",
    "    return values\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a48f31",
   "metadata": {},
   "source": [
    "# Scenario Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d39359a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Scenario</h2><ul><li><b>scenario/classname</b> =salina_cl.scenarios.classic_control.cartpole.cartpole_test</li><li><b>scenario/n_train_envs</b> =32</li><li><b>scenario/n_evaluation_envs</b> =64</li><li><b>scenario/n_tasks</b> =3</li><li><b>scenario/n_steps</b> =100000</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  4  different Hps values\n",
      "Analyzing  2  logs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Hyper-parameters</h2><ul><li><b>model/classname</b> =salina_cl.algorithms.ppo_finetune.model.PPOFineTune</li><li><b>model/params/evaluation/seed</b> =10</li><li><b>model/params/evaluation/n_rollouts</b> =10</li><li><b>model/params/evaluation/evaluate_success</b> =False</li><li><b>model/params/ppo/seed</b> =11</li><li><b>model/params/ppo/optimizer_policy/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_policy/lr</b> =0.0003</li><li><b>model/params/ppo/optimizer_critic/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_critic/lr</b> =0.0003</li><li><b>model/params/ppo/clip_grad</b> =10.0</li><li><b>model/params/ppo/control_every_n_epochs</b> =10</li><li><b>model/params/ppo/n_control_rollouts</b> =1</li><li><b>model/params/ppo/n_timesteps</b> =32</li><li><b>model/params/ppo/n_processes</b> =1</li><li><b>model/params/ppo/n_mini_batches</b> =4</li><li><b>model/params/ppo/n_envs_per_minibatch</b> =16</li><li><b>model/params/ppo/n_timesteps_per_minibatch</b> =16</li><li><b>model/params/ppo/n_times_per_minibatch</b> =1</li><li><b>model/params/ppo/discount_factor</b> =0.95</li><li><b>model/params/ppo/clip_ratio</b> =0.2</li><li><b>model/params/ppo/action_std</b> =0.4</li><li><b>model/params/ppo/gae</b> =0.95</li><li><b>model/params/ppo/reward_scaling</b> =1.0</li><li><b>model/params/ppo/time_limit</b> =0</li><li><b>model/params/ppo_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.ActionAgent</li><li><b>model/params/ppo_agent/hidden_size</b> =256</li><li><b>model/params/ppo_agent/n_layers</b> =2</li><li><b>model/params/ppo_agent/input_dimension</b> =nil</li><li><b>model/params/ppo_agent/output_dimension</b> =nil</li><li><b>model/params/critic_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.CriticAgent</li><li><b>model/params/critic_agent/hidden_size</b> =256</li><li><b>model/params/critic_agent/n_layers</b> =2</li><li><b>model/params/critic_agent/input_dimension</b> =nil</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Reward</h2><table><tr><td>Task \\ Stage </td><td><b>0</b></td><td><b>1</b></td><td><b>2</b></td></tr><tr><td><b>0</b></td><td>100.0<i>(0.0)</i></td><td>65.89453125<i>(46.08457960948764)</i></td><td>100.0<i>(0.0)</i></td></tr><tr><td><b>1</b></td><td>46.72578125<i>(0.24417281037847618)</i></td><td>45.71640625<i>(1.4992873469845995)</i></td><td>47.27265625<i>(0.5446931923827568)</i></td></tr><tr><td><b>2</b></td><td>97.23828125<i>(3.905660111710071)</i></td><td>48.28203125<i>(32.658389601708066)</i></td><td>100.0<i>(0.0)</i></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing  2  logs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Hyper-parameters</h2><ul><li><b>model/classname</b> =salina_cl.algorithms.ppo_finetune.model.PPOFineTune</li><li><b>model/params/evaluation/seed</b> =10</li><li><b>model/params/evaluation/n_rollouts</b> =10</li><li><b>model/params/evaluation/evaluate_success</b> =False</li><li><b>model/params/ppo/seed</b> =11</li><li><b>model/params/ppo/optimizer_policy/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_policy/lr</b> =0.0003</li><li><b>model/params/ppo/optimizer_critic/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_critic/lr</b> =0.0003</li><li><b>model/params/ppo/clip_grad</b> =10.0</li><li><b>model/params/ppo/control_every_n_epochs</b> =10</li><li><b>model/params/ppo/n_control_rollouts</b> =1</li><li><b>model/params/ppo/n_timesteps</b> =32</li><li><b>model/params/ppo/n_processes</b> =1</li><li><b>model/params/ppo/n_mini_batches</b> =8</li><li><b>model/params/ppo/n_envs_per_minibatch</b> =16</li><li><b>model/params/ppo/n_timesteps_per_minibatch</b> =16</li><li><b>model/params/ppo/n_times_per_minibatch</b> =1</li><li><b>model/params/ppo/discount_factor</b> =0.95</li><li><b>model/params/ppo/clip_ratio</b> =0.2</li><li><b>model/params/ppo/action_std</b> =0.4</li><li><b>model/params/ppo/gae</b> =0.95</li><li><b>model/params/ppo/reward_scaling</b> =1.0</li><li><b>model/params/ppo/time_limit</b> =0</li><li><b>model/params/ppo_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.ActionAgent</li><li><b>model/params/ppo_agent/hidden_size</b> =256</li><li><b>model/params/ppo_agent/n_layers</b> =2</li><li><b>model/params/ppo_agent/input_dimension</b> =nil</li><li><b>model/params/ppo_agent/output_dimension</b> =nil</li><li><b>model/params/critic_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.CriticAgent</li><li><b>model/params/critic_agent/hidden_size</b> =256</li><li><b>model/params/critic_agent/n_layers</b> =2</li><li><b>model/params/critic_agent/input_dimension</b> =nil</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Reward</h2><table><tr><td>Task \\ Stage </td><td><b>0</b></td><td><b>1</b></td><td><b>2</b></td></tr><tr><td><b>0</b></td><td>100.0<i>(0.0)</i></td><td>68.8734375<i>(44.01960683755379)</i></td><td>77.3625<i>(32.01425951822093)</i></td></tr><tr><td><b>1</b></td><td>45.54765625<i>(0.8938271655936237)</i></td><td>51.396093750000006<i>(6.860040631855118)</i></td><td>41.8125<i>(5.977262009717535)</i></td></tr><tr><td><b>2</b></td><td>64.22109375<i>(43.79974082277861)</i></td><td>19.92578125<i>(0.47840193164652556)</i></td><td>99.91953125<i>(0.11379999759720698)</i></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing  2  logs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Hyper-parameters</h2><ul><li><b>model/classname</b> =salina_cl.algorithms.ppo_fromscratch.model.PPOFromScratch</li><li><b>model/params/evaluation/seed</b> =10</li><li><b>model/params/evaluation/n_rollouts</b> =10</li><li><b>model/params/evaluation/evaluate_success</b> =False</li><li><b>model/params/ppo/seed</b> =11</li><li><b>model/params/ppo/optimizer_policy/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_policy/lr</b> =0.0003</li><li><b>model/params/ppo/optimizer_critic/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_critic/lr</b> =0.0003</li><li><b>model/params/ppo/clip_grad</b> =10.0</li><li><b>model/params/ppo/control_every_n_epochs</b> =10</li><li><b>model/params/ppo/n_control_rollouts</b> =1</li><li><b>model/params/ppo/n_timesteps</b> =32</li><li><b>model/params/ppo/n_processes</b> =1</li><li><b>model/params/ppo/n_mini_batches</b> =4</li><li><b>model/params/ppo/n_envs_per_minibatch</b> =16</li><li><b>model/params/ppo/n_timesteps_per_minibatch</b> =16</li><li><b>model/params/ppo/n_times_per_minibatch</b> =1</li><li><b>model/params/ppo/discount_factor</b> =0.95</li><li><b>model/params/ppo/clip_ratio</b> =0.2</li><li><b>model/params/ppo/action_std</b> =0.4</li><li><b>model/params/ppo/gae</b> =0.95</li><li><b>model/params/ppo/reward_scaling</b> =1.0</li><li><b>model/params/ppo/time_limit</b> =0</li><li><b>model/params/ppo_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.ActionAgent</li><li><b>model/params/ppo_agent/hidden_size</b> =256</li><li><b>model/params/ppo_agent/n_layers</b> =2</li><li><b>model/params/ppo_agent/input_dimension</b> =nil</li><li><b>model/params/ppo_agent/output_dimension</b> =nil</li><li><b>model/params/critic_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.CriticAgent</li><li><b>model/params/critic_agent/hidden_size</b> =256</li><li><b>model/params/critic_agent/n_layers</b> =2</li><li><b>model/params/critic_agent/input_dimension</b> =nil</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Reward</h2><table><tr><td>Task \\ Stage </td><td><b>0</b></td><td><b>1</b></td><td><b>2</b></td></tr><tr><td><b>0</b></td><td>99.99609375<i>(0.005524271728019903)</i></td><td>99.996875<i>(0.004419417382411903)</i></td><td>99.99375<i>(0.008838834764828829)</i></td></tr><tr><td><b>1</b></td><td>nan<i>(nan)</i></td><td>32.78046875<i>(10.05085998195941)</i></td><td>32.96015625<i>(10.486172594127375)</i></td></tr><tr><td><b>2</b></td><td>nan<i>(nan)</i></td><td>nan<i>(nan)</i></td><td>100.0<i>(0.0)</i></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing  2  logs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Hyper-parameters</h2><ul><li><b>model/classname</b> =salina_cl.algorithms.ppo_fromscratch.model.PPOFromScratch</li><li><b>model/params/evaluation/seed</b> =10</li><li><b>model/params/evaluation/n_rollouts</b> =10</li><li><b>model/params/evaluation/evaluate_success</b> =False</li><li><b>model/params/ppo/seed</b> =11</li><li><b>model/params/ppo/optimizer_policy/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_policy/lr</b> =0.0003</li><li><b>model/params/ppo/optimizer_critic/classname</b> =torch.optim.Adam</li><li><b>model/params/ppo/optimizer_critic/lr</b> =0.0003</li><li><b>model/params/ppo/clip_grad</b> =10.0</li><li><b>model/params/ppo/control_every_n_epochs</b> =10</li><li><b>model/params/ppo/n_control_rollouts</b> =1</li><li><b>model/params/ppo/n_timesteps</b> =32</li><li><b>model/params/ppo/n_processes</b> =1</li><li><b>model/params/ppo/n_mini_batches</b> =8</li><li><b>model/params/ppo/n_envs_per_minibatch</b> =16</li><li><b>model/params/ppo/n_timesteps_per_minibatch</b> =16</li><li><b>model/params/ppo/n_times_per_minibatch</b> =1</li><li><b>model/params/ppo/discount_factor</b> =0.95</li><li><b>model/params/ppo/clip_ratio</b> =0.2</li><li><b>model/params/ppo/action_std</b> =0.4</li><li><b>model/params/ppo/gae</b> =0.95</li><li><b>model/params/ppo/reward_scaling</b> =1.0</li><li><b>model/params/ppo/time_limit</b> =0</li><li><b>model/params/ppo_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.ActionAgent</li><li><b>model/params/ppo_agent/hidden_size</b> =256</li><li><b>model/params/ppo_agent/n_layers</b> =2</li><li><b>model/params/ppo_agent/input_dimension</b> =nil</li><li><b>model/params/ppo_agent/output_dimension</b> =nil</li><li><b>model/params/critic_agent/classname</b> =salina_cl.algorithms.ppo_finetune.agents.CriticAgent</li><li><b>model/params/critic_agent/hidden_size</b> =256</li><li><b>model/params/critic_agent/n_layers</b> =2</li><li><b>model/params/critic_agent/input_dimension</b> =nil</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Reward</h2><table><tr><td>Task \\ Stage </td><td><b>0</b></td><td><b>1</b></td><td><b>2</b></td></tr><tr><td><b>0</b></td><td>100.0<i>(0.0)</i></td><td>100.0<i>(0.0)</i></td><td>100.0<i>(0.0)</i></td></tr><tr><td><b>1</b></td><td>nan<i>(nan)</i></td><td>41.48828125<i>(1.5191747252054733)</i></td><td>40.096093749999994<i>(0.882778622137579)</i></td></tr><tr><td><b>2</b></td><td>nan<i>(nan)</i></td><td>nan<i>(nan)</i></td><td>52.7515625<i>(66.81938111343753)</i></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def analyze_runs(logs):\n",
    "    print(\"Analyzing \",len(logs),\" logs\")\n",
    "    hps=extract_hps(logs[0])\n",
    "    dfs=[]\n",
    "    for log in logs:\n",
    "        df=log.to_dataframe()\n",
    "        _cols=[c for c in df.columns if c.startswith(\"evaluation\")]+[\"iteration\"]        \n",
    "        df=df[_cols]\n",
    "        dfs.append(df)\n",
    "    \n",
    "    df=pd.concat(dfs)\n",
    "    df_mean=df.groupby(\"iteration\",as_index=False).mean()\n",
    "    df_std=df.groupby(\"iteration\",as_index=False).std()\n",
    "    columns=[c for c in df_mean.columns if not c==\"iteration\"]\n",
    "    df_mean=df_mean.dropna(subset=columns,how=\"all\")\n",
    "    df_std=df_std.dropna(subset=columns,how=\"all\")\n",
    "    n_tasks=df_mean[\"iteration\"].max()+1\n",
    "    #Collection reward\n",
    "    r_mean=np.zeros((n_tasks,n_tasks))\n",
    "    r_std=np.zeros((n_tasks,n_tasks))\n",
    "    memory_mean=np.zeros((n_tasks,))\n",
    "    memory_std=np.zeros((n_tasks,))\n",
    "    for task in range(n_tasks):\n",
    "        for stage in range(n_tasks):\n",
    "            n=\"evaluation/\"+str(task)+\"/avg_reward\"\n",
    "            \n",
    "            d=df_mean[df_mean[\"iteration\"]==stage]            \n",
    "            reward_mean=d.iloc[0][n]\n",
    "            memory_mean[stage]=d.iloc[0][\"evaluation/memory/n_parameters\"]\n",
    "            r_mean[task][stage]=reward_mean\n",
    "            \n",
    "            d=df_std[df_std[\"iteration\"]==stage]\n",
    "            reward_std=d.iloc[0][n]            \n",
    "            memory_std[stage]=d.iloc[0][\"evaluation/memory/n_parameters\"]\n",
    "            r_std[task][stage]=reward_std\n",
    "    return r_mean,r_std,memory_mean,memory_std,hps\n",
    "        \n",
    "    \n",
    "def analyze_scenario(logs,scenario):\n",
    "    h=generate_scenario_html(scenario)\n",
    "    display(HTML(h))\n",
    "    per_hps={}\n",
    "    for log in logs.logs:\n",
    "        if not has_scenario(log,scenario):\n",
    "            continue\n",
    "        h=extract_hps(log)\n",
    "        str_h=str(h)\n",
    "        if not str_h in per_hps:\n",
    "            per_hps[str_h]=[]\n",
    "        per_hps[str_h].append(log)\n",
    "    \n",
    "    print(\"Found \",len(per_hps),\" different Hps values\")\n",
    "    \n",
    "    for h in per_hps:\n",
    "        reward_mean,reward_std,memory_mean,memory_std,hps=analyze_runs(per_hps[h])\n",
    "    \n",
    "        #Generate HTML\n",
    "        h=generate_hps_html(hps)\n",
    "        display(HTML(h))\n",
    "        h=generate_reward_html(reward_mean,reward_std,)\n",
    "        display(HTML(h))\n",
    "\n",
    "for scenario in unique_scenarios(LOGS): \n",
    "    analyze_scenario(LOGS,scenario)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
